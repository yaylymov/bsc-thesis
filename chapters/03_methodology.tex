% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Methodology}\label{chapter:methodology}

\section{Selection Criteria}
The core aim of this research study involves examining and evaluating
various publicly available deepfake detection tools. It becomes imperative
to establish a well-defined set of selection criteria for these tools.
Selecting the right criteria is essential not just for representing a variety
of detection methods, but also for making a fair comparison between the tools.
The objective is to ensure that the evaluated tools are understandable,
encompassing the nuances of accessibility, ease of use, limitations, variety
in detection methods, and documentation. These are the criteria used to select the tools:

\textbf{Ease of Use and Limitations.} The tools were chosen based on their ease of
operation. Considerations included the installation process of the chosen tool,
the availability of \ac{API}, and whether a user with IT skills could utilize
these APIs for integration purposes. Questions were raised about the specific
operating systems needed, the requirement of a constant internet connection,
and any inherent limitations of the tools. Do the developer or owner of the
tool make any specific promises or claims about its use? Additionally, do the
tools have restrictions on file soze or video length, and do these factors
matter to use the tools?

\textbf{Accessibility.} Questions about the tools' accessibility include their
public availability, any special account requirements for access, and if they can
run on standard PCs or laptops. Can the tools be accessed via a website or is specialized
software platforms like GitHub, Hugging Face or Google Colab or an \ac{IDE} required?
The need for enhanced \ac{GPU} or other hardware specifications, as well as the nature
of the tool (open source vs.\ proprietary), were also considered.

\textbf{Support and Documentation.} Factors taken into account included the
provision of user support, installation guidance, usage documentation, and
resources for development. The existence of troubleshooting resources,
community discussions for Q\&A, and availability of the source code for
developers' adaptability were important considerations.

\textbf{Difficulty of Use.} The complexity of the tools was evaluated on different levels.
It was labeled as \textit{Easy} if the tool was web-based with a user-friendly
interface for detecting deepfakes. \textit{Moderate} if it provided a platform like
Hugging Face Space or \ac{API}s, and \textit{Challenging} if users had to devise their
own way to utilize the tool.

\textbf{Cost Considerations.} Determined whether the tool is free or comes with a
subscription fee, emphasizing a preference for freely available options. Also, if
paid versions existed, did they provide additional functionality or ease of use?

\textbf{Privacy Policy.} The presence of privacy policies and terms of use for the
tools was verified, underscoring their importance for tool legitimacy and
trustworthiness. Especially when users are required to log in, it's essential
to clarify how personal information is managed. Additionally, statements regarding
ethical issues and adherence to relevant laws and regulations further bolster the
tool's credibility and users' confidence in its operations.

The aforementioned criteria that were established played a critical role in the
selection process of the tools. Tools were preferred that could be easily
accessed and trusted by a broad audience. By ensuring tools had clear easy
installations and user-friendly interfaces, the safety and simplicity of the
selected tools were prioritized. It wasn't just about identifying tools that were
publicly available; the goal was to ensure the chosen tools covered not just
accessibility needs, but were also user-friendly enough for individuals without
deepfake expertise. This approach aimed to make the tools both dependable and
user-friendly, addressing a variety of user requirements.

An overview of the  selection criteria is provided in~\autoref{tab:selection_criteria}.

\begin{table}[htpb]
	\caption{Selection Criteria}\label{tab:selection_criteria}
	\centering
	\small
	\begin{tabularx}{\textwidth}{l X}
		\toprule
		\textbf{Selection Criteria} & \textbf{Description}                                      \\
		\midrule
		Ease of use and Limitations & The tool's user-friendliness is determined
		by the simplicity of its installation process and its operational
		requirements. Is it a straightforward drag-and-drop mechanism, or does
		it demand any special software? Additionally, any constraints, such
		as file size limits or video duration caps, play a role in its overall
		user-friendliness.                                                                      \\
		\addlinespace
		Accessibility               & Only publicly-available tools were considered
		to ensure broad user accessibility. The ease of use, from browser access
		to local installation, and hardware demands, with preference for
		standard configurations or cloud solutions like Google Colab,
		were pivotal in evaluations.                                                            \\
		\addlinespace
		Support and Documentation   & Proper documentation and active support,
		be it community or developer-driven, are crucial. Regular support
		ensures that users can fully make use of tool features, troubleshoot issues,
		and gain deeper insights into the tool's functionalities.                               \\
		\addlinespace
		Difficulty of Use           & The tool's complexity was categorized as
		\textit{Easy} for web-based interfaces, \textit{Moderate} for those using
		platforms like Hugging Face or offering \ac{API}s, and \textit{Challenging}
		when users needed a custom approach.                                                    \\
		\addlinespace
		Cost considerations         & It is assessed if the tool is free or
		subscription-based, favoring free options, and checked if paid versions
		offered enhanced features or usability.                                                 \\
		\addlinespace
		Privacy Policy              & Tools were evaluated for privacy policies and user terms,
		emphasizing trust. It's vital to know how tools handle login data and their
		stance on ethics and legal compliance to ensure credibility.                            \\
		\bottomrule
	\end{tabularx}
\end{table}

\section{Evaluation Metrics}
After careful selection of the tools based on the prescribed criteria, it is
important to systematically evaluate them to ensure their, accuracy
and efficiency. This evaluation isn't just about how these tools perform; it's
about understanding their strengths and potential weaknesses.

Each tool has a unique set of features and algorithms that drive its functionality.
However, to compare them on a fair level and to ensure a comprehensive
assessment, a standard set of evaluation metrics is applied. These metrics serve
as a guiding light, distinguishing the capabilities of each tool in terms of
detecting deepfakes. The evaluation metrics used in this study are provided in~\autoref{tab:evaluation_metrics}.

\begin{table}[htpb]
	\caption{Evaluation Metrics}\label{tab:evaluation_metrics}
	\centering
	\small
	\begin{tabularx}{\textwidth}{l X}
		\toprule
		\textbf{Evaluation Metrics} & \textbf{Description}                                     \\
		\midrule
		Processing time             & Measures the time taken by the tool to detect
		deepfakes in a given input. It also evaluates how well the tool performs
		when the size and the number of the input data increases.                              \\
		\addlinespace
		Interpretability            & Assesses how understandable and transparent
		the results or outputs of the tool are. It's crucial for users to comprehend
		why certain detections are made.                                                       \\
		\addlinespace
		Detection Accuracy          & The proportion of true results (both true
		positives and true negatives) in the total dataset. It provides a comprehensive
		measure of the tool's ability to correctly identify both genuine and deepfake content. \\
		\addlinespace
		Precision                   & The proportion of true positive results in the
		total predicted positives (both true and false positives). It measures the tool's capability to avoid false positives,
		ennsuring that genuine content isn't mistakenly flagged.                               \\
		\addlinespace
		Recall                      & Measures the tool's ability to correctly
		identify actual deepfakes out of all genuine deepfakes presented. It calculates the
		number of actual true positives the tool identifies.                                   \\
		\addlinespace
		F1-Score                    & The harmonic mean of Precision and Recall.
		It provides a balance between the two when there's an uneven class distribution.       \\
		\bottomrule
	\end{tabularx}
\end{table}

\section{Datasets}
Datasets are very important when working with deepfakes. They form the backbone of both
the creation and detection of deepfakes. They also help us train models to create or spot
deepfakes. Today, there are many datasets available that focus on both deepfake images
and videos. In~\autoref{fig:sample-deepfakes} a fake sample image of each dataset is provided.

\subsection{FaceForensics++}\label{section:ff++}
FaceForensics++~\footnote{\url{https://github.com/ondyari/FaceForensics}} serves as a
broad dataset designed for forensic studies. It
comprises 977 videos sourced from YouTube, over 1,000 unique sequences, and an
impressive collection of more than 8,000 Deepfake videos. Originally launched in
2018, the dataset received significant updates in 2019, making it richer and more
diverse. As highlighted by the creators in their 2019 paper~\cite{roessler2019faceforensicspp},
the videos in the dataset were modified using a mix of techniques. This includes two
graphics-driven methods, namely Face2Face and FaceSwap, as well as two methods used
in machine learning: DeepFakes and NeuralTextures. A noteworthy feature of these
manipulation methods is their reliance on both source and target video pairs for input.
This makes the dataset a valuable resource for those looking to understand the
nuances and intricacies of different deepfake generation techniques.

\subsection{Deepfake Detection Challenge Dataset}
The \ac{DFDC} dataset emerged from a collaborative initiative hosted on Kaggle~\cite{kaggle2020},
aiming to combat the rise of deceptive deepfake videos. Started in 2019 and ending
with a big competition in 2020, this challenge saw over 2200 teams competing for an
overall prize of one million dollars. This challenge wasn't just about competing.
It was a call for researchers all over the world to create new tools to spot fake
content. The dataset has 104,500 different deepfake videos from 3,426 paid
actors~\cite{dolhansky2020deepfake}. This variety makes it a great tool to test and improve deepfake detection
methods.

\subsection{Face Forensics in the Wild}
The \ac{FFIW}~\footnote{\url{https://github.com/tfzhou/FFIW}} dataset offers 10,000 high-quality
manipulated videos. What's unique about it is the automatic manipulation process. This
process is managed by a domain-adversarial quality assessment network, which means creating
this dataset requires less human intervention. This design ensures that the dataset can
be scaled up easily and at a low human cost~\cite{Zhou_2021_CVPR}.

\subsection{OpenForensics}
The OpenForensics~\footnote{\url{https://github.com/ltnghia/openforensics}} dataset is
designed for detecting and segmenting multi-face forgeries.
Its version 1.0.0 houses more than 115,000 real-world images, capturing a total of 334,000 human faces.
Each image in the dataset comes with detailed face-related annotations, like the type
of forgery, bounding boxes, segmentation masks, forgery boundaries, and typical facial
landmarks~\cite{ltnghia-ICCV2021}.


\begin{figure}[htbp]
	\centering
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{figures/method1}
		\caption{FaceForensics++}\label{fig:sub1}
	\end{subfigure}%
	\hspace{-5mm}
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{figures/method2}
		\caption{DFDC}\label{fig:sub2}
	\end{subfigure}
	\vspace{5mm} % Adjust the vertical spacing if needed
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{figures/method3}
		\caption{FFIW}\label{fig:sub3}
	\end{subfigure}%
	\hspace{-5mm}
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{figures/method4}
		\caption{OpenForensics}\label{fig:sub4}
	\end{subfigure}
	\caption{Sample deepfake images taken from FaceForensics++~\cite{roessler2019faceforensicspp},
		DFDC~\cite{dolhansky2020deepfake}, FFIW~\cite{Zhou_2021_CVPR}
		and OpenForensics~\cite{ltnghia-ICCV2021}.}\label{fig:sample-deepfakes}
\end{figure}
